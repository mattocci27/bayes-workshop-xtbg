---
title: "Practical Bayesian Modeling with Stan and brms"
author: "Masatoshi Katabuchi"
date: "`r format(Sys.time(), '%B %d, %Y')`"
fontsize: 12pt
format:
  html:
    theme: coderpro
    toc: true
    toc-depth: 2
    number-sections: true
    smooth-scroll: true
    standalone: true
    embed-resources: true
execute:
  cache: true
---

```{r global_options, include=FALSE}
library(knitr)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  cache = FALSE,
  fig.align = "center",
  fig.show = "hold"
)
```


```{r setup, mesasge=FALSE, warning=FALSE}
library(brms)
library(cmdstanr)
library(tidyverse)
library(bayesplot)
cmdstanr::set_cmdstan_path("/opt/cmdstan/cmdstan-2.36.0")
```


# Normal model

$$
y_i \sim N(\mu, \sigma)
$$

## Dummy data

```{r}
set.seed(123)
n <- 100
mu <- 5
sigma <- 2
y <- rnorm(n, mean = mu, sd = sigma)

hist(y)
```

```{r}
normal_list <- list(
  y = y,
  N = length(y)
)

normal_df <- tibble(
  y = y,
  N = length(y)
)
```

## Stan

`stan/normal.stan`

```{r, echo=FALSE, results='asis'}
cat("```stan\n", paste(readLines("stan/normal.stan"), collapse = "\n"), "\n```")
```

`stan/normal_vague.stan`

```{r, echo=FALSE, results='asis'}
cat("```stan\n", paste(readLines("stan/normal_vague.stan"), collapse = "\n"), "\n```")
```

```{r}
normal_mod <- cmdstan_model("stan/normal.stan")
normal_vague_mod <- cmdstan_model("stan/normal_vague.stan")
```

```{r}
normal_fit <- normal_mod$sample(
  data = normal_list,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 1000, # number of warmup iterations
  iter_sampling = 1000, # number of sampling iterations
  refresh = 500 # print update every 500 iters
)
```

```{r, eval=FALSE}
normal_vague_fit <- normal_vague_mod$sample(
  data = normal_list,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  refresh = 500 # print update every 500 iters
)
```

### Summary

We use this to write in manuscripts and for diagnostics.

```{r}
normal_fit$summary()
```

- `lp__`: log posterior.
  - $\text{log}\; p(\theta \mid \text{data}) \propto \text{log}\; p(\text{data} \mid \theta) + \text{log}\; p(\theta)$

- `rhat` (Gelman-Rubin statistic): should be close to 1.
  - Measures the convergence of the chains.
  - `rhat` > 1.1: Definitely bad.
  - 1.01 < `rhat` < 1.1: Suspicous.
  - `rhat` <=  1.01: Good.

- `ess_bulk` and `ess_tail`: Effective sample size for bulk and tail of the posterior distribution.
  - `ess_bulk`: sampling efficiency for the bulk posteriors (e.g., mean, SD).
  - `ess_tail`: sampling efficiency for the tails (e.g., quantiles like 2.5%, 97.5%).
  - `ess_bulk`, `ess_tail` > 400: Good.


### Draws (posterior samples)

For each parameter, we have 1000 iterations $\times$ 4 chains = 4000 posteriors.
We use this for visualization and diagnostics.

```{r}
normal_fit$draws()
```


Trace plots for diagnosing convergence and mixing of the chains.
```{r}
normal_draws <- as_draws_df(normal_fit$draws())

color_scheme_set("viridis")
mcmc_trace(normal_draws, pars = c("mu", "sigma"))
```

Histograms of the posterior distributions of the parameters.

```{r}
mcmc_hist(normal_draws, pars = c("mu", "sigma"))
```


### Diagnostic summary

```{r}
normal_fit$diagnostic_summary()
```

- `num_divergent`: indicates the number of iterations (sampling transitions) where the Hamiltonian trajectory failed — meaning Stan detected a **divergent transition**.
  - Even 1-2 divergent transitions suggest that model may not be reaiable, especially in hierachical models.
  - We need reparamaterization or increase `adapt_delta` to make the sampler take smaller steps.

- `num_max_treedepth`: indicates the number of iterations where there are not enough leafprog steps.
  - This can indicate that the model is complex or that the priors are too tight.
  - We can increase `max_treedepth` to allow more steps, but this can increase the computation time.

- `ebfmi`: Energy-Bayesian Fraction of Missing Information
  -	Measures whether the resampled momentum generates enough variation in energy to explore the posterior efficiently.
  - Low `ebfmi` means the sampler may be stuck in tight regions and exploring poorly, even if Rhat and ESS look fine.
  - Guidelines:
    - `ebfmi` < 0.3: Bad
    - 0.3 < `ebfmi` <= 1: Acceptable
    - `ebfmi` >= 1: Good

## `brms`

```{r, eval=FALSE}
normal_fit_brm0 <- brm(y ~ 1, family = gaussian(), data = normal_df)
```

```{r normal-brm}
normal_fit_brm <- brm(
  y ~ 1,
  family = gaussian(),
  data = normal_df,
  prior = c(
    prior(normal(0, 5), class = "Intercept"),
    prior(cauchy(0, 2.5), class = "sigma")
  )
)
```

### Summary

```{r}
summary(normal_fit_brm)
```

### Draws (posterior samples)

```{r}
normal_draws_brm <- posterior::as_draws_df(normal_fit_brm)
normal_draws_brm
```

```{r}
mcmc_trace(normal_draws_brm, pars = c("b_Intercept", "sigma"))
mcmc_hist(normal_draws_brm, pars = c("b_Intercept", "sigma"))
```

### Diagnostic summary

```{r}
rstan::check_hmc_diagnostics(normal_fit_brm$fit)
```

# Poisson model

$$
y_i \sim \text{Poisson}(\lambda_i)
$$

$$
y_i \sim \operatorname{Poisson\_log}(\log \lambda_i)
$$

## Dummy data

```{r}
set.seed(123)
n <-  100
y <- rpois(n, lambda = 5)

pois_list <- list(
  y = y,
  N = n
)

pois_df <- tibble(
  y = y,
  N = n
)
```

<!-- poisson_mod <- cmdstan_model("stan/poisson.stan") -->

## Stan

`stan/pois.stan`

```{r, echo=FALSE, results='asis'}
cat("```stan\n", paste(readLines("stan/pois.stan"), collapse = "\n"), "\n```")
```

`stan/pois_re.stan`

```{r, echo=FALSE, results='asis'}
cat("```stan\n", paste(readLines("stan/pois_re.stan"), collapse = "\n"), "\n```")
```

```{r}
pois_mod <- cmdstan_model("stan/pois.stan")
pois_re_mod <- cmdstan_model("stan/pois_re.stan")
```

```{r}
pois_fit <- pois_mod$sample(
  data = pois_list,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 1000, # number of warmup iterations
  iter_sampling = 1000, # number of sampling iterations
  refresh = 0 # print update every 500 iters
)
```

```{r}
pois_re_fit <- pois_re_mod$sample(
  data = pois_list,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 1000, # number of warmup iterations
  iter_sampling = 1000, # number of sampling iterations
  refresh = 0 # print update every 500 iters
)
```

```{r}
pois_fit$summary()
```

```{r}
pois_re_fit$summary()
```


# Multiple linear model

$$
y_i \sim N(\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i}, \sigma)
$$

or

$$
y_i \sim N(\boldsymbol{\beta \cdot x}, \sigma)
$$


```{r}
set.seed(123)
n <- 100
beta <- c(2, 1.2, -0.8)
x1 <- rnorm(n, mean = 0, sd = 1)
x2 <- rnorm(n, mean = 0, sd = 1)
sigma <- 0.4
y <- rnorm(n, mean = beta[1] + beta[2] * x1 + beta[3] * x2, sd = sigma)
```

```{r}
lm_list <- list(
  y = y,
  N = length(y),
  x = rbind(1, x1, x2)
)

lm_df <- tibble(
  y = y,
  N = length(y),
  x1 = x1,
  x2 = x2
)
```

## Stan

`stan/lm.stan`

```{r, echo=FALSE, results='asis'}
cat("```stan\n", paste(readLines("stan/lm.stan"), collapse = "\n"), "\n```")
```

```{r lm-model}
lm_mod <- cmdstan_model("stan/lm.stan")

lm_fit <- lm_mod$sample(
  data = lm_list,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 1000, # number of warmup iterations
  iter_sampling = 1000, # number of sampling iterations
  refresh = 0 # don't print update
)

lm_fit$summary()
```

## brms

```{r lm-brm}
lm_fit_brm <- brm(
  y ~ x1 + x2,
  family = gaussian(),
  data = lm_df,
  prior = c(
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 2.5), class = "b"),
    prior(cauchy(0, 2.5), class = "sigma")
  ),
  refresh = 0 # don't print update
)

summary(lm_fit_brm)
```

# Varying intercepts

Leaf nitrogen contents increase with soil nitrogen concentration, but the baseline leaf nitrogen contents differ among species.

$$
y_{ij} \sim N(\beta_{0j} + \beta_1 x_{i}, \sigma)
$$

$$
\beta_{0j} \sim N(\hat{\beta_0}, \tau)
$$

$$
\hat{\beta_0} \sim N(0, 2.5)
$$

$$
\tau \sim \text{Half-Cauchy}(0, 1)
$$


## dummy data

```{r}

library(tidyverse)

set.seed(123)

# ---- Define parameters ----
J              <- 6     # number of species
n_per_species  <- 25    # individuals per species
true_beta1     <- 0.8   # true slope for nitrogen effect
true_sigma     <- 1.5   # residual SD

# Hyperpriors
hat_beta0      <- rnorm(1, 0, 2.5)       # grand intercept ~ N(0,2.5)
tau            <- abs(rcauchy(1, 0, 1))  # species‐sd ~ Half‐Cauchy(0,1)

# Draw species‐level intercepts
beta0_j <- rnorm(J, mean = hat_beta0, sd = tau)

# Optional: assign real species names
species_names <- paste0("Sp", LETTERS[1:J])

# ---- Simulate data ----
dummy_data <- tibble(
  species  = factor(rep(species_names, each = n_per_species)),
  nitrogen = rnorm(J * n_per_species, mean = 4, sd = 1.5)  # soil N
) %>%
  mutate(
    # map each species to its intercept
    beta0 = beta0_j[as.integer(species)],
    # generate the response (leaf area)
    leaf_area = rnorm(n(), mean = beta0 + true_beta1 * nitrogen, sd = true_sigma)
  ) %>%
  select(species, nitrogen, leaf_area)

# ---- Glimpse ----
dummy_data %>%
  group_by(species) %>%
  summarise(
    mean_leaf_area = mean(leaf_area),
    sd_leaf_area   = sd(leaf_area)
  )
# Varying slopes and intercepts

# Varying slopes and intercepts, and group-level predictors
